Increasing the energy efficiency of data centers in the tropics is not impossible, but it is harder, and with significantly lesser gains than similar facilities located in locations with temperate climates.
Yet tropical Southeast Asia is also home to two-third of a billion people, with surging digital growth that is increasing rapidly. As more data centers are being built in the region, are there innovations or practical strategies that can help data center providers go green here?
Professor PS Lee, Program Director, Cooling Energy Science and Technology, National University of Singapore
For Professor PS Lee, the program director of Cooling Energy Science and Technology at the National University of Singapore (NUS), a hybrid liquid cooling solution might be the solution that the region needs. On a video call, Lee walked DCD through a hybrid cooling system that he developed for data centers.
The solution entails utilizing liquid cooling to target active components such as the CPU, GPU, and RAM, and relying on standard rack- or hall-level air cooling for the rest of the server. Much like current water-cooling solutions used by PC gaming enthusiasts, the coolant is contained within a closed loop, effectively eliminating water loss. Lee takes this further, feeding the water to a heat exchanger at the rack level, which transfers the heat to an outdoor dry cooler via a second loop.
When tested under loads of 32kW, Lee says his hybrid liquid cooling solution achieved a PUE of just 1.091 – with the advantage of being far easier to implement than full immersion liquid cooling. And because 95 percent of the heat is already removed via liquid cooling, Lee says his solution can allow for data centers to be built without chillers and CRAC units. Without these systems and the raised floors that typically come with them, the result is significantly less M&E work, lower costs, and faster fit-out of data centers.
But getting a greenfield data center built without chillers is a difficult sell in most parts of the world, and more so in tropical locations. However, a surprise finding in new tests conducted at AI Singapore earlier this year prompted Lee to commercialize his hybrid liquid cooling solution for existing facilities.
With a single-rack testbed consisting of 20 servers with one AMD Epyc processor and four Nvidia RTX2080 GPUs each, Lee says his hybrid cooling solution recorded a staggering reduction in IT power of at least 25 percent. The sharp lowering of junction temperature on the microprocessors contributed to the big drop in IT power consumption, Lee explained. This was possible due to the use of liquid cooling, combined with a unique, high efficiency “oblique fin” cold plate that he designed.
This means that existing data centers can deploy Lee’s solution to benefit from significantly lower power consumption. By removing the second cooling loop and expelling the waste heat via standard rear door heat exchangers, no building modifications would be required. Lee says he is currently in advanced talks with at least two operators in the region about his solution.
So why has hybrid liquid cooling not found greater use? Lee postulated that this could be due to the majority of equipment vendors originating from the United States or Europe. With lower hanging fruits such as ambient temperature cooling, there might be less urgency to develop cooling systems geared specifically towards deployments in tropical climates.
But even standard systems deployed well can make a world of difference, according to Darren Hawkins, CEO of SpaceDC. Building an efficient data center boils down to the design, he told DCD in a call. The Singapore-based provider is currently constructing a data center campus in Indonesia that is scheduled to launch in the second half of this year.
For the 2.6MW JAK1 and 24MW JAK2, SpaceDC is eschewing data center specifications more commonly found in the region in favor of fan walls and continuous cooling. Façade elements will be installed to reduce solar heat gains to a minimum, says Hawkins. Of course, a larger data center also allows for a lower carbon footprint due to factors such as economies of scale, a lower PUE, and fewer people required for security and maintenance.
A common mistake is underestimating the importance of the right skillset for deployed systems: “Skillset is especially important, especially with the complex, large data centers that we are building. For example, it’s no use [for us] getting someone familiar with split system cooling.”
“You need to ensure that the technology that you deploy is operated and maintained at its peak performance. Your technology is your enabler. But if your operations team is not familiar with it, then that could lead to further issues.”
Going for the common denominator is probably not on his team’s mind, considering how SpaceDC’s upcoming facilities do not use raised floors, and don’t even rely on the power grid for normal operations. Instead, JAK1 and JAK2 will use a natural gas-driven reciprocating engine to power the data center campus.
Hawkins says this less common choice was made after a thorough audit of the power sources available in Jakarta, its quality, history of supply, the available capacity, and how it is distributed. “We were easily able to identify the grid has multiple issues each month. They are not necessarily blackouts – but low voltages and high harmonics. This means that data centers here will typically go to generator power,” he said.
In view of this, delivering the most resilient data center design meant adapting with an on-site power plant. As a bonus, the gas-powered generator means the presence of absorption chillers to recycle waste heat for the chillers, while the cleaner power from the generators translate to a more efficient energy chain with equipment like UPS units running in high-efficiency mode.
Hawkins likens this ability to adapt to building data centers: “We provide what [hyperscalers] want in terms of continuous cooling, contiguous space, and reliable power. We take that and make it into a building that is adapted for the local climate. Requirements are the same, but how you provide it is very different. This ability to translate and deliver across different cities, not just cope, but to perform.”
With new innovations and a willingness to adapt, there is no reason why data center operators in tropical climates cannot still build energy efficient facilities.
