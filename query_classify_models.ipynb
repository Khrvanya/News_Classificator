{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape_queries import *\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "MODELS_PATH = os.path.join(CURRENT_PATH, 'models')\n",
    "BASIC_LINKS_AMOUNT = 300\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, name: str, queries=None, links_number=None, children_set=None):\n",
    "        if children_set is None:\n",
    "            children_set = set()\n",
    "        if queries is None:\n",
    "            queries = [name]\n",
    "        if links_number is None:\n",
    "            links_number = [BASIC_LINKS_AMOUNT] * len(queries)\n",
    "            \n",
    "        self.name = name\n",
    "        self.queries = to_list(queries)\n",
    "        self.links_number = to_list(links_number)    \n",
    "        self.children_set = children_set        # set of Nodes or None\n",
    "        self.pipeline = None\n",
    "        \n",
    "    def get_children_names(self):\n",
    "        if self.children_set == None:\n",
    "            return None\n",
    "        name_set = set()\n",
    "        for node in self.children_set:\n",
    "            name_set.add(node.name)\n",
    "        return name_set\n",
    "    \n",
    "    def get_children_queries(self):\n",
    "        queries_set = set()\n",
    "        for node in self.children_set:\n",
    "            queries_set.add(', '.join(node.queries))\n",
    "        return queries_set\n",
    "        \n",
    "    def __str__(self):\n",
    "        print('name: ', self.name, '\\nqueries: ', self.queries,\n",
    "              '\\nlinks number: ', self.links_number,\n",
    "              '\\nchildren names: ', self.get_children_names(), \n",
    "             '\\npipeline: ', self.pipeline)\n",
    "        return '\\n'\n",
    "\n",
    "def make_node_structure() -> Node:\n",
    "    \"\"\"\n",
    "    Makes a Node structure, which looks like a tree\n",
    "    Returns the tree root\n",
    "    \"\"\"\n",
    "\n",
    "    agricult = Node('agricult', ['agriculture'], children_set={Node('cattle'), Node('corn'), \n",
    "                                              Node('soybean'), Node('sugar')})\n",
    "    \n",
    "    bitcoin = Node('bitcoin', ['bitcoin', 'bitcoin crypto', '\"bitcoin\"'], [300, 300, 300])\n",
    "    dash = Node('dash', ['dash', 'dash crypto', '\"dash\"'], [300, 300, 300])\n",
    "    ethereum = Node('ethereum', ['ethereum', 'ethereum crypto', '\"ethereum\"'], [300, 300, 300])\n",
    "    litecoin = Node('litecoin', ['litecoin', 'litecoin crypto', '\"litecoin\"'], [300, 300, 300])\n",
    "    monero = Node('monero', ['monero', 'monero crypto', '\"monero\"'], [300, 300, 300])\n",
    "    ripple = Node('ripple', ['ripple', 'ripple crypto', '\"ripple\"'], [300, 300, 300])\n",
    "    zimbocash = Node('zimbocash', ['zimbocash', 'zimbocash crypto', '\"zimbocash\"'], [120, 120, 300])\n",
    "    \n",
    "    cryptocurrency = Node('cryptocurrency', children_set={bitcoin, dash, ethereum, \n",
    "                                                  litecoin, monero, ripple, zimbocash})\n",
    "    \n",
    "    energy = Node('energy', ['energy'], children_set={Node('brent crude'), Node('coal'),\n",
    "                    Node('crude oil'), Node('natural gas')})\n",
    "    \n",
    "    metals = Node('metals', ['metals'], children_set={Node('gold'), Node('iron'),\n",
    "                    Node('platinum'), Node('silver')})\n",
    "                    \n",
    "    finance = Node('finance', ['finance'], children_set={agricult, cryptocurrency, energy, metals})\n",
    "    \n",
    "    return cryptocurrency                             ####\n",
    "\n",
    "\n",
    "def get_vectorizers(feature=None) -> set:\n",
    "    \"\"\"\n",
    "    Return list of text vectorizers\n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = None\n",
    "    \n",
    "    if feature == 'all':\n",
    "        vectorizer = {(\"cv\", CountVectorizer(stop_words=\"english\")), \n",
    "                       (\"tfidfv\", TfidfVectorizer(stop_words=\"english\"))}\n",
    "    elif feature == 'cv':\n",
    "        vectorizer = {(\"cv\", CountVectorizer(stop_words=\"english\"))}\n",
    "    elif feature == 'tfidfv':\n",
    "        vectorizer = {(\"tfidfv\", TfidfVectorizer(stop_words=\"english\"))}\n",
    "    elif feature == None:\n",
    "        vectorizer = set()\n",
    "    \n",
    "    assert not vectorizer == None, '!!!given feature is wrong!!!'\n",
    "    \n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "def get_classificators(feature=None) -> set:\n",
    "    \"\"\"\n",
    "    Return list of classificators\n",
    "    \"\"\"\n",
    "    \n",
    "    classifier = None\n",
    "    \n",
    "    if feature == 'all':\n",
    "        classifier = {(\"rf\", RandomForestClassifier(n_jobs=-1, random_state=17)), \n",
    "                       (\"logreg\", LogisticRegression(n_jobs=-1, random_state=17))}\n",
    "    elif feature == 'rf':\n",
    "        classifier = {(\"rf\", RandomForestClassifier(n_jobs=-1, random_state=17))}\n",
    "    elif feature == 'logreg':\n",
    "        classifier = {(\"logreg\", LogisticRegression(n_jobs=-1, random_state=17))}\n",
    "    \n",
    "    assert not classifier == None, '!!!given feature is wrong!!!'\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "\n",
    "def get_models_params(feature=None) -> dict:\n",
    "    \"\"\"\n",
    "    Return dict of models params\n",
    "    \"\"\"\n",
    "    \n",
    "    param_dict = {}\n",
    "    \n",
    "    if feature == 'all' or feature == 'rf':\n",
    "        rf_params = {\n",
    "            'rf__n_estimators': [50, 100, 200],\n",
    "            'rf__max_depth': [15, None],\n",
    "            'rf__max_features': [.5, .7, 1]\n",
    "        }\n",
    "        param_dict['rf'] = rf_params\n",
    "        \n",
    "    if feature == 'all' or feature == 'logreg':\n",
    "        logreg_params = {\n",
    "            'logreg__C': np.logspace(-3, 4, 7),\n",
    "        }\n",
    "        param_dict['logreg'] = logreg_params\n",
    "    \n",
    "    if feature == 'all' or feature == 'cv':\n",
    "        cv_params = {\n",
    "            'cv__ngram_range': [(1, 1), (1, 2)],\n",
    "            'cv__max_features': [300, 500, 1000, 1500, 2000]\n",
    "        }\n",
    "        param_dict['cv'] = cv_params\n",
    "    \n",
    "    if feature == 'all' or feature == 'tfidfv':\n",
    "        tfidfv_params = {\n",
    "            'tfidfv__ngram_range': [(1, 1), (1, 2)],\n",
    "            'tfidfv__max_features': [300, 500, 1000, 1500, 2000]\n",
    "        }\n",
    "        param_dict['tfidfv'] = tfidfv_params\n",
    "    \n",
    "    \n",
    "    assert (not param_dict == {}) or feature == None, '!!!given feature is wrong!!!'\n",
    "    \n",
    "    return param_dict\n",
    "\n",
    "\n",
    "def find_best_pipeline(X, y, classificators: str, vectorizers=None, parameters=None):\n",
    "    \"\"\"\n",
    "    Finds best model for data with vectorizer(not neccessary) and classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                        random_state=17)\n",
    "    \n",
    "    classifiers = get_classificators(classificators)\n",
    "    vectorizers = get_vectorizers(vectorizers)\n",
    "    params_dict = get_models_params(parameters)\n",
    "    \n",
    "    pipelines_scores = []\n",
    "    \n",
    "    for classifier in classifiers:\n",
    "        vectorizers_copy = copy.deepcopy(vectorizers)  \n",
    "        \n",
    "        while True:\n",
    "            step = []\n",
    "\n",
    "            if vectorizers_copy:\n",
    "                step.append(vectorizers_copy.pop())\n",
    "            step.append(classifier)\n",
    "            pipeline = Pipeline(step)\n",
    "            \n",
    "            params = {}\n",
    "            for element in step:\n",
    "                try:\n",
    "                    params.update(params_dict[element[0]])\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            \n",
    "            grid = GridSearchCV(pipeline, params, cv=5, n_jobs=-1)            \n",
    "            grid.fit(X_train, y_train)\n",
    "            pipelines_scores.append((grid.best_score_, grid))\n",
    "            \n",
    "            if not vectorizers_copy:\n",
    "                break\n",
    "                \n",
    "    best_grid = sorted(pipelines_scores, key=lambda x: -x[0])[0][1]\n",
    "    best_pipe = best_grid.best_estimator_\n",
    "    mask = best_grid.cv_results_['rank_test_score'] - 1\n",
    "    best_cv_std = best_grid.cv_results_['std_test_score'][mask][0]\n",
    "    \n",
    "#     assert best_cv_std < .1, '\\n!!!std is bigger than 0.1!!!\\n'                          #####\n",
    "    \n",
    "    print('!!!best pipeline steps: ', '-'.join(list(best_pipe.named_steps.keys())), '!!!')\n",
    "    print('!!!best pipeline params: ', best_grid.best_params_, '!!!\\n')\n",
    "    print('!!!best mean and std cross-val score: ', best_grid.best_score_, ', ', best_cv_std, '!!!')\n",
    "    print('!!!test score: ', best_pipe.score(X_test, y_test), '!!!\\n')\n",
    "\n",
    "    return best_pipe\n",
    "\n",
    "\n",
    "def train_best_pipeline(queries_path: str, categories: list):\n",
    "    \"\"\"\n",
    "    Makes and trains a model for a node\n",
    "    Returns pipe of models that are trained\n",
    "    \"\"\"\n",
    "    assert categories, '!!!categories list is empty!!!'\n",
    "\n",
    "    full_data_text = load_files(queries_path, categories=categories, \n",
    "                                encoding=\"utf-8\", decode_error=\"replace\", random_state=17)\n",
    "    \n",
    "    labels, counts = np.unique(full_data_text.target, return_counts=True)\n",
    "    labels_sort = np.array(full_data_text.target_names)[labels]\n",
    "    print('\\n!!!making pipeline for node: \\n', dict(zip(labels_sort, counts)), '!!!\\n')\n",
    "    \n",
    "    X_text = [process_text(text) for text in full_data_text.data]\n",
    "    y_text = full_data_text.target\n",
    "    \n",
    "    pipeline = find_best_pipeline(X_text, y_text, 'all', 'all', 'all')\n",
    "    pipeline.fit(X_text, y_text)\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def make_classification_models(root_node, models_path, queries_path, save=False,\n",
    "                               delete_previous={'models': False, 'scrape': False}):\n",
    "    \"\"\"\n",
    "    Makes classification models for root structure using \n",
    "    scrape_articles.py\n",
    "    \"\"\"\n",
    "    \n",
    "    if not root_node.children_set:\n",
    "        return None\n",
    "    print('!!!start making', root_node.name, 'pipeline!!!')\n",
    "    \n",
    "    for node in root_node.children_set:\n",
    "        if delete_previous['scrape'] or not os.path.exists(os.path.join(queries_path,\n",
    "                                                                    ', '.join(node.queries))):\n",
    "            scrape_query_news_articles(node.queries, node.links_number, queries_path)\n",
    "    \n",
    "    if delete_previous['models'] or not re.findall(root_node.name, \n",
    "                                                   ' '.join(os.listdir(models_path))):\n",
    "                                                       \n",
    "        trained_pipeline = train_best_pipeline(queries_path, root_node.get_children_queries())\n",
    "        \n",
    "        joblib.dump(trained_pipeline, os.path.join(models_path, root_node.name + '.sav'))\n",
    "                                                       \n",
    "        if save:\n",
    "            root_node.pipeline = trained_pipeline\n",
    "    \n",
    "    print('!!!end making', root_node.name, 'pipeline!!!\\n\\n')\n",
    "    for node in root_node.children_set:\n",
    "        make_classification_models(node, models_path, queries_path, save, delete_previous)    \n",
    "\n",
    "\n",
    "# make_classification_models(make_node_structure(), MODELS_PATH, QUERIES_PATH, \n",
    "#                            delete_previous={'models': True, 'scrape': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "# kernel works but nothing happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_text = load_files(QUERIES_PATH, categories=make_node_structure().get_children_queries(), \n",
    "                                encoding=\"utf-8\", decode_error=\"replace\", random_state=17)\n",
    "    \n",
    "labels, counts = np.unique(full_data_text.target, return_counts=True)\n",
    "labels_sort = np.array(full_data_text.target_names)[labels]\n",
    "print('\\n!!!making pipeline for node: \\n', dict(zip(labels_sort, counts)), '!!!\\n')\n",
    "    \n",
    "X_text = [process_text(text) for text in full_data_text.data]\n",
    "y_text = full_data_text.target\n",
    "\n",
    "params = {\n",
    "            'rf__n_estimators': [50, 100],\n",
    "        }\n",
    "\n",
    "grid = GridSearchCV(Pipeline([('tfidf', TfidfVectorizer(stop_words=\"english\")), ('rf', RandomForestClassifier(random_state=17))]), \n",
    "                             params, cv=5, n_jobs=-1)            \n",
    "grid.fit(X_text, y_text)\n",
    "\n",
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
